{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danilomennezes/LLM-PLN/blob/main/LLM_Chatgpt_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXaFkTnh0IoB"
      },
      "source": [
        "# LLM - Apresentação API ChatGPT\n",
        "\n",
        "Apresntação da API do ChatGPT para alunos do Curso de Ciencias da Computação da UNESP de São José do Rio Preto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQUVctKrTYvA"
      },
      "source": [
        "https://openai.com/chatgpt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QZzAjZ-QBgx"
      },
      "outputs": [],
      "source": [
        "token = 'xxxxxxxxxxxxxxxxxxx'#Substitua o 'xxxxxxxxxxxxxxxxxxx' com o seu token, adquirido em https://platform.openai.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqFGeDABQBgt"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "openai.api_key = token #passando o token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Primeiro Exemplo (Documentação)\n",
        "\n",
        "https://platform.openai.com/docs/overview"
      ],
      "metadata": {
        "id": "rr9PChnwEc-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8GQaxL9JbYN"
      },
      "outputs": [],
      "source": [
        "#Apresentando modelo disponivel na documentação API\n",
        "from openai import OpenAI\n",
        "#Instanciando o modelo  chat.completions da OpenaAI\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTGGr109J8xx"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51VTewEs7dSZ"
      },
      "source": [
        "## Exemplo Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxgbh7gh7h9U"
      },
      "outputs": [],
      "source": [
        "#modelo Completion - Gerar texto com base em uma entrada fornecida\n",
        "response = openai.completions.create(\n",
        "  model=\"gpt-3.5-turbo-instruct\",\n",
        "  prompt=\"Como fazer uma limonada\",\n",
        "  max_tokens=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOinCxNg7ooi"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8TD-8QKy-cN"
      },
      "outputs": [],
      "source": [
        "#as respostas retonadas geralmente vem em formato JSON, Para visualizar o formato importamos essa biblioteca\n",
        "import json\n",
        "response_dict = response.to_dict()\n",
        "response_json = json.dumps(response_dict, indent=4)\n",
        "\n",
        "# Exiba o JSON formatado\n",
        "print('Resposta em formato JSON:')\n",
        "print(response_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKnrNgka8lPD",
        "outputId": "82e49f0b-7b6e-447c-9f88-100d8c01fcb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'response' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a55397596a0e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresposta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresposta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
          ]
        }
      ],
      "source": [
        "resposta = (response.choices[0].text)\n",
        "print(resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9kKnz_87xwB"
      },
      "source": [
        "## Exemplo chat.completion\n",
        "\n",
        "Modelo que ajuda a criar diálogos mais claros e eficazes, podendo passar instruções ao modelo através dos 'Role'.\n",
        "\n",
        "Role User - Este é o papel do usuário que faz perguntas ou solicitações.\n",
        "\n",
        "Role System - Passa informações ao sistema de como ele deve respnder as perguntas.\n",
        "\n",
        "Role Assistant - Geralmente utilizado para guardar a conversa , como um assistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfgUYjP-eobZ"
      },
      "outputs": [],
      "source": [
        "#Modelo chatComletion - Gerar conversa mantendo o contexto\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\", #modelo da OpenAI\n",
        "  messages=[ {\"role\": \"system\",  \"content\": \"Você receberá declarações, e sua tarefa é convertê-las para o português.\"},\n",
        "             {\"role\": \"user\",  \"content\": \"how are you.\" }], #lista de mensagens que definem o contexto da conversa.\n",
        "  temperature=0.0, #Controla a aleatoriedade da resposta de 0 a 1, sendo 0 mais previsivel e 1 mais criativo.\n",
        "  max_tokens=100, #Define o número máximo de tokens\n",
        "  n = 1, #numero de resposta do modelo\n",
        "  frequency_penalty= 1, #penaliza a repetição de palavras, varia de -2 a 2 quanto mais próximo de 2, menor será repetição de palavras\n",
        "  presence_penalty = 1, #penaliza a introdução de novos tópicos, varia de -2 a 2 quanto mais próximo de 2, menor será a introdução de novos tópicos\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L9MdrC38P5x"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "response_dict = response.to_dict()\n",
        "response_json = json.dumps(response_dict, indent=4)\n",
        "\n",
        "# Exiba o JSON formatado\n",
        "print('Resposta em formato JSON:')\n",
        "print(response_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0C2Nn72z50M"
      },
      "outputs": [],
      "source": [
        "resposta = response.choices[0].message.content\n",
        "print('Sua Resposta:', resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Zg3gEx_caE"
      },
      "source": [
        "###assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YwLm0Kq-hks"
      },
      "outputs": [],
      "source": [
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[ {\"role\": \"assistant\",  \"content\": resposta},\n",
        "             {\"role\": \"user\", \"content\": \"traduza para o ingles novamente\" } ],\n",
        "  temperature=0.7,\n",
        "  max_tokens=100,\n",
        "  top_p=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e28DdkK6-hyu"
      },
      "outputs": [],
      "source": [
        "resposta = response.choices[0].message.content\n",
        "print('Sua Resposta:', resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS5HJnzKz-8u"
      },
      "source": [
        "# Iniciando a Aplicação\n",
        "Desenvolveremos um modelo, que ira identicar a duvida do usuário e irá escolher a melhor configuração de 'SYSTEM' para encaminar para o GPT-3.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33mDS17xi-i7"
      },
      "outputs": [],
      "source": [
        "#Criando uma variavel do tipo input para receber a iteração.\n",
        "sistema = input('Deseja falar com um Médico, Advogado ou Programador?')\n",
        "\n",
        "#Criando um prompt especifico para cada especialidade.\n",
        "if sistema.lower == 'Médico':\n",
        "  prompt = [{\"role\": \"system\", \"content\": \"voce é um Médico especialista.\"},\n",
        "              {\"role\": \"system\", \"content\": \"inicie falando sobre sua especilidade.\"}\n",
        "\n",
        "              ]\n",
        "elif sistema.lower == 'Advogado':\n",
        "  prompt = [{\"role\": \"system\", \"content\": \"voce é um Advogado especialista.\"},\n",
        "              {\"role\": \"system\", \"content\": \"inicie falando sobre sua especilidade.\"}\n",
        "\n",
        "              ]\n",
        "elif sistema.lower == 'Programador':\n",
        "  prompt = [{\"role\": \"system\", \"content\": \"voce é um Programdor especialista.\"},\n",
        "              {\"role\": \"system\", \"content\": \"inicie falando sobre sua especilidade.\"}\n",
        "              ]\n",
        "else:\n",
        "  prompt = [ {\"role\": \"system\", \"content\": \"inicie falando sobre sua especilidade.\"}\n",
        "              ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFJ5GjqHi0F7"
      },
      "outputs": [],
      "source": [
        "#Criando uma função para armazenar a instancia da API do GPT\n",
        "def chamar_openai_chat(prompt):\n",
        "  response = openai.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=prompt,\n",
        "    temperature=1,\n",
        "    max_tokens=500)\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GA20QIoUtXp"
      },
      "outputs": [],
      "source": [
        "#Passando o prompt\n",
        "pergunta = input('O que deseja saber?:')\n",
        "\n",
        "#While para manter a conversa\n",
        "while pergunta.lower()!= 'sair':\n",
        "  prompt.append({\"role\": \"user\", \"content\": pergunta})\n",
        "  resposta = chamar_openai_chat(prompt) #chamada a função 'chamar_openai_chat'\n",
        "  prompt.append({\"role\": \"assistant\", \"content\": resposta}) #guardando a resposta dentro da variavel prompt, para passar como um assistente para o GPT\n",
        "  print(f\"Resposta Chat: \\n, {resposta} \\n\")\n",
        "  pergunta = input('O que deseja saber?: ')\n",
        "print(\"Até logo!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyP9u8FBKxag"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNrS9CFYmcZdju/rz3inwRB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}