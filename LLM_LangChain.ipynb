{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMzig+WdoTpjgWaYJCL1uA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danilomennezes/LLM-PLN/blob/main/LLM_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM - Apresentação LangChain\n",
        "\n",
        "Apresntação de LangChain para alunos do Curso de Ciencias da Computação da UNESP de São José do Rio Preto."
      ],
      "metadata": {
        "id": "YXaFkTnh0IoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.langchain.com/\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/tutorials/llm_chain/"
      ],
      "metadata": {
        "id": "0C-AAPH32uHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install langchain-community\n",
        "from langchain_openai import OpenAI, ChatOpenAI #importando os pacotes OpenAI dentro da LangChain\n",
        "\n",
        "import os #bibliota que cria variaveis de ambiente\n",
        "\n",
        "token = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' #Substitua o 'xxxxxxxxxxxxxxxxxxx' com o seu token, adquirido em https://platform.openai.com/\n",
        "os.environ['OPENAI_API_KEY'] = token #Inserindo tokem\n"
      ],
      "metadata": {
        "id": "E76Q8O3G1x3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Primeiro exemplos\n"
      ],
      "metadata": {
        "id": "DQzTanFz2iuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modelo Completion - Gerar texto com base em uma entrada fornecida\n",
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct') #Instanciando o modelo, através da classe OpenAI dentro da biblioteca de LangChain\n",
        "response = openai.invoke(input='segunda guerra mundial?',\n",
        "                         temperature=1,\n",
        "                         max_tokens=100, ) #funcão invoke faz a chamada do modelo e passa os input e recebe o output\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QHasP3Mm1x9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo chatComletion - Gerar conversa mantendo o contexto\n",
        "#Classe ChatOpenAI\n",
        "openai = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Você é um assistente especialista em história.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Segunda guerra mundial??\"}\n",
        "]\n",
        "response = openai.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "uVnNIpEf1yAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Memory\n",
        "A memória em LangChain é uma ferramenta para gerenciar e manter o contexto em aplicações baseadas em modelos de linguagem. Permite que o sistema armazene e recupere informações relevantes ao longo de interações, melhorando a coesão e a personalização das respostas.\n",
        "\n",
        "Ainda permite consultas a perguntas repetidas, buscando em sua memória para evitar custos com a API\n",
        "\n"
      ],
      "metadata": {
        "id": "gqR2Df7X3z6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache"
      ],
      "metadata": {
        "id": "UV9gL35h8g_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplo 1 - Guardando na memória perguntas repetitivas afim de evitar custos com consultas ao GPT-3.5\n",
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature =  1, max_tokens=50)\n",
        "set_llm_cache(InMemoryCache())"
      ],
      "metadata": {
        "id": "UbK1h83C3CG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_1 = openai.invoke('Como fazer uma limonada')\n",
        "print(\"Primeira Resposta: \", resposta_1)"
      ],
      "metadata": {
        "id": "0RGaFv483CQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ao repetir a pergunta, o modelo não faz a consulta ao GPT-3.5, pois a mesma já se encontra em sua memória\n",
        "resposta_2 = openai.invoke('Como fazer uma limonada')\n",
        "print(\"Segunda Resposta:\", resposta_2)"
      ],
      "metadata": {
        "id": "skiexFsV3CYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplo 2 - Guardando a conversa em memória para dar sentio e contexto\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "rBFrrzXZ1yFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9_d0zOb-Dwa"
      },
      "outputs": [],
      "source": [
        "openai = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0,max_tokens = 150)\n",
        "memory = ConversationBufferMemory() #Instância para armazenar e recuperar o histórico da conversa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cria uma instância de ConversationChain, configurada com o modelo de linguagem openai e o mecanismo de memória memory.\n",
        "conversation = ConversationChain( llm=openai,  memory = memory, verbose= True)"
      ],
      "metadata": {
        "id": "4C-oIbZwbdo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Olá! como fazer uma limonada?\")"
      ],
      "metadata": {
        "id": "AubfaISHcaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='mas vai ficar muito azeda?' )"
      ],
      "metadata": {
        "id": "W_huXNtRbkMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1cpcg6-S5_P"
      },
      "outputs": [],
      "source": [
        "memory.buffer #Verificando a memória"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Template\n",
        "Um template em LangChain é uma forma de definir um padrão ou esqueleto para a criação de texto, que pode ser preenchido com dados variáveis.\n",
        "\n",
        "Ajuda a criar prompts ou outras partes de texto de maneira mais estruturada e reutilizável, permitindo que você defina como as informações devem ser organizadas e apresentadas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5D7DocPjCTKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI,OpenAI\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "caK0EMO1CPyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define uma variavel com o template, passando os campos variaveis\n",
        "template = '''\n",
        "           Você é um analista do mercado financeiro.\n",
        "           Faça um breve analise dos seguintes itens {itens} da empresa \"{empresa}\"\n",
        "           Leve em consideração o período {periodo}.\n",
        "'''"
      ],
      "metadata": {
        "id": "RwFfqHt2CQDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Prompt_Template = PromptTemplate.from_template(template=template) #Instancia o modelo"
      ],
      "metadata": {
        "id": "mZ5ZM_lTCQNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instânciando o Prompt Template\n",
        "prompt = Prompt_Template.format(\n",
        "    empresa='ITUB4',\n",
        "    periodo='Primeiro trimestre de 2024',\n",
        "    itens='PVP e IBITDA /n preco sob'\n",
        ")\n",
        "print(\"ChatGPT\\n\", prompt)"
      ],
      "metadata": {
        "id": "Foqf8kpmCQmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chamar o modelo LangChain chatgpt\n",
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct', max_tokens=2000)\n",
        "\n",
        "response = openai.invoke(prompt)\n",
        "print(\"ChatGPT\\n\", response)"
      ],
      "metadata": {
        "id": "yQpwqQ7jCQtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chains - Router Chains\n",
        "Sequenciar chamadas, que serão utilizadas de maneira encadeada (Definir etapas)\n",
        "\n",
        "Sequencial chains: Utilizam sequencias para um atendimento, podendo fazer uma consulta no manual do produto e outra consulta no registro de atendimentos, enviar ambas ao GPT e criar uma resposta final.\n",
        "\n",
        "Router chains:Definem rotas que serão executada de acordo com o inout de entrada\n"
      ],
      "metadata": {
        "id": "1WmI-GbRJFlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n"
      ],
      "metadata": {
        "id": "jVA2cKIBJlzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciando o modelo da OpenAI\n",
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0)"
      ],
      "metadata": {
        "id": "YzIjWDztJl3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolveremos um modelo, que será capaz de classificar uma pergunta e logo após acionar o template especifico."
      ],
      "metadata": {
        "id": "O9QE1um64n8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_clacificar = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"    Classifique a pergunta do usuário em uma das seguintes categorias:\n",
        "              - medico\n",
        "              - juridico\n",
        "              - financeiro\n",
        "              - tecnologia\n",
        "              - Outras Informações\n",
        "\n",
        "              Pergunta: {query}\n",
        "              Classificação:\n",
        "              \"\"\" )\n",
        "    | openai\n",
        "    | StrOutputParser() #garante que a saida será uma string\n",
        ")"
      ],
      "metadata": {
        "id": "hgqiN7u6Jl7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplo fazendo a chamada em GPT para classificar\n",
        "item_classificado = template_clacificar.invoke({\"query\": \"processo de execução fiscal\"})\n",
        "print(item_classificado)"
      ],
      "metadata": {
        "id": "HxCADZ78X_U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(template_clacificar)"
      ],
      "metadata": {
        "id": "dxUrvNQKJmIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chains sequencial para cada topico\n",
        "chain_medico = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Você é um médico.\n",
        "    Comece a resposta com \"Ola, sou um Médico\" e logo após um resumo da pergunta do usuario.\n",
        "    Responda à pergunta do usuário:\n",
        "    Pergunta: {query}\n",
        "    Resposta:\n",
        "    \"\"\"\n",
        ") | openai\n",
        "chain_advogado = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Você é um advogado.\n",
        "    Comece a resposta com \"Ola, sou um Advogado\" e logo após um resumo da pergunta do usuario.\n",
        "    Responda à pergunta do usuário:\n",
        "    Pergunta: {query}\n",
        "    Resposta:\n",
        "    \"\"\"\n",
        ") | openai\n",
        "chain_analista_financeiro = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Você é um analista financeiro.\n",
        "    Comece a resposta com \"Ola, sou um Analista Financeiro\" e logo após um resumo da pergunta do usuario.\n",
        "    Responda à pergunta do usuário:\n",
        "    Pergunta: {query}\n",
        "    Resposta:\n",
        "    \"\"\"\n",
        ") | openai\n",
        "\n",
        "chain_analista_ti = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Você é um analista de TI.\n",
        "    Comece a resposta com \"Ola, sou um Analista de TI\" e logo após um resumo da pergunta do usuario.\n",
        "    Ajude o usuário com seu problema técnico.\n",
        "    Pergunta: {query}\n",
        "    Resposta:\n",
        "    \"\"\"\n",
        ") | openai\n",
        "\n",
        "chain_outras_informacoes = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Você é um assistente de informações gerais.\n",
        "    Comece a resposta com \"Ola, sou um analista Geral\" e logo após um resumo da pergunta do usuario.\n",
        "    Forneça informações ao usuário sobre sua pergunta.\n",
        "    Pergunta: {query}\n",
        "    Resposta:\n",
        "    \"\"\"\n",
        ") | openai"
      ],
      "metadata": {
        "id": "cPB4zd_XJmPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de roteamento\n",
        "def route(info):\n",
        "    topic = info[\"topic\"].lower()\n",
        "    if \"financeiro\" in topic:\n",
        "        return chain_analista_financeiro\n",
        "    elif \"medico\" in topic:\n",
        "        return chain_medico\n",
        "    elif \"juridico\" in topic:\n",
        "        return chain_advogado\n",
        "    elif \"tecnologia\" in topic:\n",
        "        return chain_analista_ti\n",
        "    else:\n",
        "        return chain_outras_informacoes"
      ],
      "metadata": {
        "id": "DlCAXhxFX_Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Input = input(\"Ola, o que deseja ?\")"
      ],
      "metadata": {
        "id": "7ZtE4VEEdnTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perguntas\n",
        "item_classificado = template_clacificar.invoke({\"query\": Input })\n",
        "prompt = route({\"topic\": item_classificado})\n",
        "\n",
        "response = prompt.invoke({\"query\":Input})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "HwYeIJmYcRFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "31XksNwMd3BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tools\n",
        "\n",
        "Tools são ferramentas úteis que auxiliam os modelos de linguagem, permitindo integrações.\n",
        "\n",
        "Estas ferramentas realizam determinadas ativadades, como buscar dados na internet, Wikipia, fazer analise ou calculos\n",
        "\n"
      ],
      "metadata": {
        "id": "zvpPAymrdveZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental\n",
        "from langchain_experimental.utilities import PythonREPL #ferramenta python\n",
        "!pip install duckduckgo-search\n",
        "from langchain_community.tools import DuckDuckGoSearchRun,WikipediaQueryRun #busca web\n",
        "!pip install wikipedia\n",
        "from langchain_community.utilities import WikipediaAPIWrapper #busca Wikipedia"
      ],
      "metadata": {
        "id": "a8XnXJlJezeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplo1 - Exemplo isolados\n",
        "#fazendo a requisão para Python de calculos\n",
        "python_repl = PythonREPL()\n",
        "result = python_repl.run(\"print(50/10)\") #utiliza a instancia do objto run\n",
        "print(result)"
      ],
      "metadata": {
        "id": "bJszYGh0ezlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fazendo a requisão para duckduckgo de busca na WEB\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "search_results = ddg_search.run(\"segunda guerra mundial? traduza para o portugues\")\n",
        "print(search_results)"
      ],
      "metadata": {
        "id": "T_1vV2rnezot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fazendo a requisão para Wikipedia\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "query = \"segunda guerra mundial\"\n",
        "wikipedia_results = wikipedia.run(query)\n",
        "print(\"Wikipedia:\", wikipedia_results)"
      ],
      "metadata": {
        "id": "Lme79llVezr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agentes\n",
        "\n",
        "São componentes que utilizam modelos de linguagem para tomar decisões, executar ações e gerar respostas com base em uma série de condições e regras. Eles são projetados para atuar como intermediários entre o modelo de linguagem e outras partes do sistema, como APIs, bancos de dados ou interfaces de usuário.\n",
        "\n"
      ],
      "metadata": {
        "id": "VOwauUEMe3nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent #Agentes python\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "tyuq2g82fHF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0, max_tokens=200)"
      ],
      "metadata": {
        "id": "td5ybQpRfHI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciando o DuckDuckGo\n",
        "ddg_search = DuckDuckGoSearchRun()"
      ],
      "metadata": {
        "id": "aD_fenKLfHLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando o Agente (create_python_agent), em llm passamos o openai, em ferramenta passamos a busca na web DuckDuckGo, verbose true para debugar\n",
        "agent_executor = create_python_agent(llm=openai, tool=ddg_search, verbose=True)\n",
        "\n",
        "#Criando o Template de Prompt passando a variavel query\n",
        "prompt_template = PromptTemplate( input_variables=[\"query\"],\n",
        "                                  template=\"\"\" Pesquise na web sobre {query} e forneça um resumo. \"\"\")"
      ],
      "metadata": {
        "id": "IK406-RhfHPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"segunda guerra mundial\"\n",
        "prompt = prompt_template.format(query=query)\n",
        "print(f\"prompt:\\n{prompt}\")"
      ],
      "metadata": {
        "id": "SslCUnkqj1Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Executar o agente, passando o prompt\n",
        "response = agent_executor.invoke(prompt)\n",
        "#Imprimindo a resposta\n",
        "print(f\"return: \\n\", response['output'])"
      ],
      "metadata": {
        "id": "tgbcadpfj1W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##React Agents\n",
        "\n",
        "React Agentes é uma extensão dos agentes simples, introduzindo a capacidade de reagir e adaptar-se a mudanças, consegue determinar qual a melhor maneira de atender ao prompt, podendo ser capaz de utilizar diversas tools"
      ],
      "metadata": {
        "id": "a5KFZwGul524"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "lP_gB4YVn4A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0, max_tokens=2000)"
      ],
      "metadata": {
        "id": "sCMeiO1gnTfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolveremos um modelo, que será capaz de entender uma pergunta e ter autonomia para buscar a emlhor forma de responder.\n",
        "\n",
        "Neste exemplo o modelo utilizara uma ferramenta para cálculo e outra para buscar sugestões na internet.\n"
      ],
      "metadata": {
        "id": "RxZUbNjK5l_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt\n",
        "prompt = ''' Sempre responda, informando o saldo, fazendo um resumo da pergunta e passando dicas de finanças.\n",
        "             Pergunta: {pergunta}'''\n",
        "prompt_template = PromptTemplate.from_template(prompt)\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "GVIu0YkVnCRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instruções - Template do Hub da LangChain, com instruções de como o agente deve se comportar\n",
        "react_instructions  = hub.pull('hwchase17/react')"
      ],
      "metadata": {
        "id": "KLreC3d4nC0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando as Tools - Ferramentas que serão utilizadas para que o nosso modelo possa responder a pergunta do usuário.\n",
        "#Passar regras claras para o prompt. Um prompt bem definido pode diminuir a possibilidade de alucinação do GPT.\n",
        "\n",
        "# Ferramenta calculo Python REPL\n",
        "python_repl = PythonREPLTool() #ferramenta para executar código python (podendo fazer calculo)\n",
        "python_repl_tool = Tool( name='Python REPL',\n",
        "                         func=python_repl.run, ##run para executar esta ferramenta\n",
        "                         description='''Para calculos, utilize esta ferramenta com código python.'''\n",
        "\n",
        ")\n",
        "\n",
        "# Ferramenta  busca DuckDuckGo\n",
        "search = DuckDuckGoSearchRun() #ferramenta para fazer buscas na WEB\n",
        "duckduckgo_tool = Tool(\n",
        "    name='Busca DuckDuckGo',\n",
        "    func=search.run,\n",
        "    description='''Para encontrar informacoes na internet, utilize esta ferramenta'''\n",
        ")"
      ],
      "metadata": {
        "id": "IXPJXOSonC8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciando o AgentExecutor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=create_react_agent(openai, [python_repl_tool, duckduckgo_tool], react_instructions),\n",
        "    tools=[python_repl_tool, duckduckgo_tool],\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True, #controlar como o agente deve lidar com erros\n",
        "    max_iterations=10 #define o número máximo de iterações que o agente pode realizar, para evitar loop\n",
        ")\n",
        "print(agent_executor)"
      ],
      "metadata": {
        "id": "XoStXRiTozvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_pergunta = \"\"\"\n",
        "Minha renda mensal é de 7500, o total de minhas despesas mensais é R$2500 mais 1000 de aluguel.\n",
        "Poderia me dar algumas dicas de investimento?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QGeokl_fo0Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = agent_executor.invoke({\n",
        "    'input': prompt_template.format(pergunta=prompt_pergunta)\n",
        "})"
      ],
      "metadata": {
        "id": "-hgGDcu_reth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['input'])"
      ],
      "metadata": {
        "id": "uP3HYNmpre6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['output'])"
      ],
      "metadata": {
        "id": "ILxhbp7wrfWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}